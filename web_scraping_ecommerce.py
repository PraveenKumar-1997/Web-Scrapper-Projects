# -*- coding: utf-8 -*-
"""Web Scraping eCommerce.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11dh0b_Br6PdaMkqJJK35EaiX3BFsoNGe
"""

import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
import requests
from tqdm import tqdm_notebook as tqdmn
from random import randint

# pip install selenium
EXECUTABLE_PATH  = r'C:/Users/RBG305/Downloads/chromedriver'

RANGE = 51 # 81
SHEET_NAME = 'Kitchen and Dining' # 'Home & Kitchen'

from bs4 import BeautifulSoup
from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# options = Options()
# options.headless = True
skus, units_per_cases, names, pr_descriptions, urls, img_urls, categorizations, brands, upcs = [],[],[],[],[],[],[],[],[]
base_url = 'https://www.dollardays.com'

# ---------#
# To not to load images and javascript (speed execution)
option = webdriver.ChromeOptions()
prefs = {'profile.default_content_setting_values': {'images':2, 'javascript':2}}
option.add_experimental_option('prefs', prefs)
# ---------#

driver = webdriver.Chrome(EXECUTABLE_PATH, options=option)
    
#for url in tqdmn(mel.Url, leave=False)
for i in tqdmn(range(0, RANGE+1), leave=False):
    #driver = webdriver.Chrome(options=options, executable_path=EXECUTABLE_PATH)
    URL = 'https://www.dollardays.com/wholesale-kitchen-and-dining-pg' + str(i+1) + '.html' # CHANGE IT /////////^^^^
    #driver.get(URL)
    #print ("Headless Chrome Initialized")
    #driver.quit()
    
   
    # # driver = webdriver.Chrome(executable_path=EXECUTABLE_PATH)
    driver.get(URL)

    soup = BeautifulSoup(driver.page_source, features='html.parser')
    driver.quit()

    links_in_this_page = []
    imgs = soup.findAll("div", {"class":"prod-img"})

    for img in imgs:
        link = img.find('a').get('href')
        print(base_url + link)
        links_in_this_page.append(base_url + link)

    for url in links_in_this_page:
        
        driver = webdriver.Chrome(executable_path=EXECUTABLE_PATH,  options=option)
        #driver.set_page_load_timeout(0.8)
        driver.get(url)

        soup = BeautifulSoup(driver.page_source, features='html.parser')
        driver.quit()

        try:
            cat_path_with_name = soup.find_all('ul', {'class': 'breadcrumb col-sm-12'})[0].text
        except:
            cat_path_with_name = soup.find_all('ul', {'class': 'breadcrumb col-sm-12'}).text

        first_path = cat_path_with_name.split('\n')[1:][:-1]
        needed_path = ' | '.join(first_path[:-1])
        product_name = first_path[-1]
        print(needed_path)
        print(product_name)
        categorizations.append(needed_path) # Categorization
        names.append(product_name)          # Name
        urls.append(url)                    # current url

        skus_and_units = soup.find_all('ul', {'class': 'sku_cod'})[0].text
        units_per_case = skus_and_units.split('\n')[1:][:2][1].split(':')[1].strip() # Units per case
        sku = skus_and_units.split('\n')[1:][:2][0].replace('SKU #', 'prem-') # SKU
        units_per_cases.append(units_per_case)
        skus.append(sku)

        try:
            upc = soup.find("div", {"id":"ctl00_cphContent_divUPC"}).text.replace('UPC:', 'prem-') # UPC
        except:
            upc = 'None'
        upcs.append(upc)

        brand_string = soup.find("div", {"class":"color_dic"}).text.strip()
        a,b = brand_string.find('Brand:'), brand_string.find('See more')
        brand = brand_string[a+6:b].strip()  # Brand
        brands.append(brand)

        # PARAGRAPHS
        ps = []
        descriptions = soup.find_all('div', {'class': 'right_content_info'})
        for para in descriptions:
            for p in para.findAll('p'):
                ps.append(p.text)
        paras = ' '.join(ps).strip()

        # Bullets in Description
        bls = []
        bullets_in_description = soup.find_all('ul', {'class': 'bullet'})
        for bullet in bullets_in_description:
            for li in bullet.findAll('li'):
                print(li.text)
                bls.append(li.text)
        blets_in_description = ' '.join(bls)    

        # FINAL PR Descrption
        pr_desc = paras + blets_in_description
        pr_descriptions.append(pr_desc)

        # Image URL
        index = soup.find('img', {"class": "main-image thumb_box"})['src'].find('?auto')
        img_url = 'https:' + soup.find('img', {"class": "main-image thumb_box"})['src'][:index]
        img_urls.append(img_url)
        print(img_url)

df = pd.DataFrame()
df['sku'] = skus
df['Units per case'] = units_per_cases
df['Name'] = names
df['URL'] = urls
df['Description'] = pr_descriptions
df['Img_URL'] = img_urls
df['Categorization'] = categorizations
df['Brand'] = brands
df['upc'] = upcs

df.to_excel(SHEET_NAME + '.xlsx', index=False)